# Audio scripts

https://www.youtube.com/watch?v=q5yxIzs5Wug

Communication is most effective when we can use the maximum amount of different kinds of communication channels, that have the maximum amount of detail. We already use body language and facial expressions to communicate our emotions and stances toward each other. It's very important to understand what the other person knows and how they feel about certain things. We can't persuade someone if we use words they don't understand, or words they hate. We should use words they love, and words they understand. And for that, we need to notice their emotional expressions and when specifically they start and when they end, to understand what caused it. Just one emotion is not enough, an entire conversation with a full range of emotions about the same word is required to fully understand how they use that word, why, and what words usually come next.

If you're reading this, you're probably using English to understand how I use the word "understand", why, and what words usually come next. That's pretty fucking neat. And the word "fucking" probably causes some emotions in you. Those emotions are replays of all the times you heard the word "fucking" before, mixed together. So to understand the meaning of the words I just used, you must have seen someone else use them before, with the same emotions, facial expressions, and body language.

Most programming languages are also based on English, and follow the same rules. If we want to understand the words in our program uses, why don't we use some equivalent of emotions, facial expressions, and body language to understand our codebases? After all, they're just stories written by another person, for both you and the computer to read and understand.

Our brains are wired to notice sequential patterns with high fidelity, especially sound. We hear someone use their mouth to produce a coherent stream of highly varied and short vocalizations, and we recognize the meaning of the spoken words faster than we can consciously think. We notice slight differences in the way things are pronounced as accents, dialects, or lithps. We listen to fast-paced music with more than five instruments playing at the same time. Okay, maybe not you or I specifically, but some people do. The point is that we have a powerful capacity to recognize patterns in sequences of events, especially in audio.

This means that if we generate audio based on the structure of our program, we could analyze it more easily. We make our program emit a specific sound when something important happens, and listen to the sequence of sounds. We can then compare two sequences and find differences and similarities. We can notice differences in rhythm, pitch, or volume. It would be easy to notice both high- and low-level patterns in the audio stream. It would be easy to filter out sounds made by different sources, at different loudnesses, and in different stereo locations. If we used this high dynamic range to analyze events in our program, we would get a lot of useful information about the patterns in our code in a very short amount of time.

To that end, in this debugger, you can write a script to play certain sounds at specific points of the execution, the same way you can write scripts to gather data that can later be displayed visually. You can replay and zoom in on specific parts of the generated track, and you can filter out specific sounds. You can write different scripts for different subsystems, or analyze different aspects of the same system. You can compare different versions of the same audio script, generated before and after a change in the code, and you can notice any bugs by listening to music. You can even notice similar melodies from different codebases.

This approach has already been used to visualize sorting algorithms, especially when combined with visual animations ((example here)[https://www.youtube.com/watch?v=LOZTuMds3LM]). The same could be done for search algorithms, or any algorithms, in any combination.

The ability to hear your code being executed makes it easier to understand the overall structure of your code and the order of events. It also lets you understand multithreaded code and catch race conditions. You can use a set of two sounds that should always happen in a specific order, like someone preparing to sneeze, and the loud sneeze itself, to instantly recognize on a subconscious level when something that shouldn't be physically possible just happened. You can also use popular tunes like the Happy Birthday song, or a jingle from an ad you can't get out of your head. You will instantly recognize when something is wrong. You will test program correctness by listening to a song.

[How is this different from visual animations?]

If you're stuck on a particularly annoying bug, you can entertain yourself by writing a program that creates an interesting structure and assign sounds to specific parts of that structure, and make a song to build up motivation to solve the actual bug. It's even easier than making a song using programming languages like SuperCollider or Csound created for the specific purpose of making songs programmatically.

This feature makes the debugger a lot more fun to use, and opens up new ways of understanding your code.